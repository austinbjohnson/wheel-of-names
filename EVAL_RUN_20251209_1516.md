# Evaluation Run Setup - 20251209_1516

## âœ… Structure Created

```
evaluations/
  â””â”€â”€ 20251209_1516_results.md  âœ“ Created with template

results/
  â””â”€â”€ 20251209_1516/            âœ“ Created (empty, ready for HTML files)

screenshots/
  â””â”€â”€ 20251209_1516/            âœ“ Created (empty, ready for screenshots)

prompts/
  â””â”€â”€ v3.md                     âœ“ Updated "Used in runs" field
```

## ðŸ“‹ Next Steps for Evaluation

### 1. Generate HTML Files
For each model, save the generated HTML to:
- `results/20251209_1516/claude-4.5-opus.html`
- `results/20251209_1516/claude-4.5-sonnet.html`
- `results/20251209_1516/claude-4.5-haiku.html`
- `results/20251209_1516/gpt-5.1-codex-high-fast.html`
- `results/20251209_1516/gemini-3-pro.html`
- `results/20251209_1516/composer-1.html`
- `results/20251209_1516/grok-code-fast-1.html`

### 2. Capture Screenshots
For each model, capture and save screenshots to `screenshots/20251209_1516/`:
- `{model}_screenshot.png` - Initial page load
- `{model}_wheel.png` - Wheel with names added
- `{model}_spinning.png` - Wheel mid-spin
- `{model}_winner.png` - Winner modal with confetti
- `{model}_disabled.png` - After disabling a winner (if applicable)
- `{model}_error.png` - Console errors or broken state (if any)

Model shorthand: `opus`, `sonnet`, `haiku`, `gpt`, `gemini`, `composer`, `grok`

### 3. Fill in Evaluation Results
Edit `evaluations/20251209_1516_results.md` with:
- Scores for each category
- Detailed notes
- Key findings
- Conclusions

## ðŸŽ¯ Prompt Being Used

**Version:** v3 (Dark Mode)  
**Location:** `prompts/v3.md`  
**Focus:** Dark mode theme with vibrant accents

## ðŸ“Š Evaluation Rubric

See `EVALUATION_RUBRIC.md` for scoring criteria.

---

**Timestamp:** 2025-12-09 15:16  
**Status:** Ready for evaluation âœ…




